{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69783e2a-c384-f515-f521-91ee392225f0"
   },
   "source": [
    "# Quoras Question Pairs Modeling Notebook\n",
    "\n",
    "This notebook try to predict if some pair of Quoras questions are duplicated or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "786ab221-5b7a-f0bf-3fba-b7e28a85f1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from zipfile import ZipFile\n",
    "from time import time\n",
    "from numpy import empty\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6afe8b8b-0f11-84b3-5366-84c9ee806472"
   },
   "source": [
    "First, lets get the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "bd969c66-57a9-a5ab-9e01-c425f75b38cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5dfdeab7-d1de-715a-aa34-3f38390d74c7"
   },
   "outputs": [],
   "source": [
    "texts = df_train[['question1','question2']]\n",
    "labels = df_train['is_duplicate']\n",
    "\n",
    "del df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82174075-3d4b-4411-9c26-c8395a82fad9"
   },
   "source": [
    "Now, lets build our model. First we need tokenize the questions to create a word index, then we use a embedding layer that transforms the input vector, our sentences in terms of words index, to dense vectors that represents in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "db64171b-73ff-6274-1e3e-f250813a5c92"
   },
   "outputs": [],
   "source": [
    "# Model params\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "# Train params\n",
    "NB_EPOCHS = 1\n",
    "BATCH_SIZE = 1024\n",
    "VAL_SPLIT = 0.1\n",
    "WEIGHTS_PATH = 'lstm_weights.h5'\n",
    "SUBMIT_PATH = 'lstm_submission_1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74339d5d-5527-3d1f-d5b1-7b317b85294c"
   },
   "source": [
    "Prepare the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6c4f94f7-f399-0513-e6d3-0e08c8b6540f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/Keras-2.0.0-py3.6.egg/keras/preprocessing/text.py:89: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring Test Data\n",
      "Done! Acquisition time: 5.820836067199707\n",
      "Preprocessing test data\n",
      "Preprocessed Batch 100/2291, Word index size: 107704, ETC: 420.59292912483215 seconds\n",
      "Preprocessed Batch 200/2291, Word index size: 115958, ETC: 490.5876259803772 seconds\n",
      "Preprocessed Batch 300/2291, Word index size: 121623, ETC: 498.6466920375824 seconds\n",
      "Preprocessed Batch 400/2291, Word index size: 125845, ETC: 505.8784236907959 seconds\n",
      "Preprocessed Batch 500/2291, Word index size: 129007, ETC: 527.0521574020386 seconds\n",
      "Preprocessed Batch 600/2291, Word index size: 131381, ETC: 541.9919347763062 seconds\n",
      "Preprocessed Batch 700/2291, Word index size: 133093, ETC: 441.7637176513672 seconds\n",
      "Preprocessed Batch 800/2291, Word index size: 134319, ETC: 507.5879158973694 seconds\n",
      "Preprocessed Batch 900/2291, Word index size: 135225, ETC: 390.2722487449646 seconds\n",
      "Preprocessed Batch 1000/2291, Word index size: 135885, ETC: 352.63199067115784 seconds\n",
      "Preprocessed Batch 1100/2291, Word index size: 136304, ETC: 308.8143661022186 seconds\n",
      "Preprocessed Batch 1200/2291, Word index size: 136553, ETC: 285.8056755065918 seconds\n",
      "Preprocessed Batch 1300/2291, Word index size: 136735, ETC: 293.95811009407043 seconds\n",
      "Preprocessed Batch 1400/2291, Word index size: 136855, ETC: 231.90428709983826 seconds\n",
      "Preprocessed Batch 1500/2291, Word index size: 136924, ETC: 222.18228316307068 seconds\n",
      "Preprocessed Batch 1600/2291, Word index size: 136978, ETC: 210.7723445892334 seconds\n",
      "Preprocessed Batch 1700/2291, Word index size: 137010, ETC: 165.05656242370605 seconds\n",
      "Preprocessed Batch 1800/2291, Word index size: 137024, ETC: 190.54534888267517 seconds\n",
      "Preprocessed Batch 1900/2291, Word index size: 137032, ETC: 115.96355557441711 seconds\n",
      "Preprocessed Batch 2000/2291, Word index size: 137037, ETC: 94.16053891181946 seconds\n",
      "Preprocessed Batch 2100/2291, Word index size: 137040, ETC: 55.913952350616455 seconds\n",
      "Preprocessed Batch 2200/2291, Word index size: 137042, ETC: 26.978177547454834 seconds\n",
      "Done! Preprocessing time: 655.389386177063\n",
      "Word index length: 137042\n",
      "Shape of data tensor: (404290, 25) (404290, 25)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tk = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "\n",
    "tk.fit_on_texts(list(texts.question1.values.astype(str)) + list(texts.question2.values.astype(str)))\n",
    "x1 = tk.texts_to_sequences(texts.question1.values.astype(str))\n",
    "x1 = pad_sequences(x1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2 = tk.texts_to_sequences(texts.question2.values.astype(str))\n",
    "x2 = pad_sequences(x2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Preprocessing Test\n",
    "print(\"Acquiring Test Data\")\n",
    "t0 = time()\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "print(\"Done! Acquisition time:\", time()-t0)\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing test data\")\n",
    "t0 = time()\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    if (i*BATCH_SIZE > df_test.shape[0]):\n",
    "        break\n",
    "    t1 = time()\n",
    "    tk.fit_on_texts(list(df_test.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE].question1.values.astype(str))\n",
    "                    + list(df_test.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE].question2.values.astype(str)))\n",
    "    i += 1\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Preprocessed Batch {0}/{1}, Word index size: {2}, ETC: {3} seconds\".format(i,\n",
    "                                                                int(df_test.shape[0]/BATCH_SIZE+1),\n",
    "                                                                len(tk.word_index),\n",
    "                                                                int(int(df_test.shape[0]/BATCH_SIZE+1)-i)*(time()-t1)))\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "print(\"Done! Preprocessing time:\", time()-t0)\n",
    "print(\"Word index length:\",len(word_index))\n",
    "\n",
    "print('Shape of data tensor:', x1.shape, x2.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ddc62929-dbec-de8d-7f06-b0523e849252"
   },
   "source": [
    "Our model,\n",
    " this time, is a siamese deep neuronet, each \"head\" with a embedding layer, some time distributed dense layers along the sequence axis, the a sum layer that agregate the interpretations of the time distributed dense layers, a concatenation layer for the heads and finnaly, a dense neuronet with sigmoid activation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "0d9dd8cc-7f48-d949-a63a-c587115172cf"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Lambda, TimeDistributed, PReLU, Merge, Activation, Embedding\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "def get_model(p_drop=0.0):\n",
    "    encoder_1 = Sequential()\n",
    "    encoder_1.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    encoder_1.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "    encoder_1.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(EMBEDDING_DIM,)))\n",
    "\n",
    "    encoder_2 = Sequential()\n",
    "    encoder_2.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    encoder_2.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))\n",
    "    encoder_2.add(Lambda(lambda x: K.sum(x, axis=1), output_shape=(EMBEDDING_DIM,)))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Merge([encoder_1, encoder_2], mode='concat'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(EMBEDDING_DIM))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(p_drop))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(EMBEDDING_DIM))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(p_drop))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f132274a-5f61-81aa-5dcb-87019c2f4af1"
   },
   "source": [
    "Usually is a good idea to search over some parms space for the optimum hyper parameters. Here we do this with gridsearch using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "193ae836-bdb9-b93e-dbd8-2efd111bdde6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nfrom sklearn.model_selection import GridSearchCV\\n\\nprint(\"Searching for optimum hyper parameters.\")\\nt0 = time()\\nmodel = KerasClassifier(build_fn=get_model, verbose=0)\\n\\n# define the grid search parameters\\nbatch_size = [128, 256, 512, 1024, 2048]\\np_drop = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\\nparam_grid = dict(batch_size=batch_size, p_drop=p_drop)\\n\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\\ngrid_result = grid.fit([x1, x2], labels)\\n\\n# summarize results\\nprint(\"Best: {0} using {1}\".format(grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nprint(\"Done! Time elapsed:\", time()-t0)\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Searching for optimum hyper parameters.\")\n",
    "t0 = time()\n",
    "model = KerasClassifier(build_fn=get_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [128, 256, 512, 1024, 2048]\n",
    "p_drop = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "param_grid = dict(batch_size=batch_size, p_drop=p_drop)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit([x1, x2], labels)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: {0} using {1}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "print(\"Done! Time elapsed:\", time()-t0)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "'''\n",
    "    \n",
    "# Usually, this is the code for gridsearch a keras model with sklearn, however for the merged model,\n",
    "# i got this error. As I can't find a solution for this error on the web and don't have the time to dig\n",
    "# deeper, I'll appreciate to hear your insights about how to do it, if you have some to share!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "a4fc1139-6020-93c1-993e-684cfa6d041b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:25: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/opt/conda/lib/python3.6/site-packages/Keras-2.0.0-py3.6.egg/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/1\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.7276Epoch 00000: val_acc improved from -inf to 0.76895, saving model to weights.h5\n",
      "363861/363861 [==============================] - 264s - loss: 0.5380 - acc: 0.7276 - val_loss: 0.4745 - val_acc: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94a88657f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(p_drop=0.2)\n",
    "checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "model.fit([x1, x2], y=labels, batch_size=384, nb_epoch=1,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ba307e3c-ea01-3ba2-4dd0-cfe9639b7650"
   },
   "source": [
    "As kaggle have time limit for running kernels, this models trains just one epoch and is pretty small. A bigger/depper model with proper training time will perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8ea13e05-2c2c-3cbe-65f3-d923afd39289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Batch 1000/2290, ETC: 91.08523726463318 seconds\n",
      "Predicted Batch 2000/2290, ETC: 17.796544790267944 seconds\n",
      "Done!\n",
      "Submission file saved to: lstm_submission_1.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "#print(\"Loading best trained model\")\n",
    "#model = load_model(WEIGHTS_PATH)\n",
    "\n",
    "# Predicting\n",
    "i = 0\n",
    "predictions = empty([df_test.shape[0],1])\n",
    "while True:\n",
    "    t1 = time()\n",
    "    if (i * BATCH_SIZE > df_test.shape[0]):\n",
    "        break\n",
    "    x1 = pad_sequences(tk.texts_to_sequences(\n",
    "        df_test.question1.iloc[i * BATCH_SIZE:(i + 1) * BATCH_SIZE].values.astype(str)), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    x2 = pad_sequences(tk.texts_to_sequences(\n",
    "        df_test.question2.iloc[i * BATCH_SIZE:(i + 1) * BATCH_SIZE].values.astype(str)), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    try:\n",
    "        predictions[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = model.predict([x1, x2], batch_size=BATCH_SIZE, verbose=0)\n",
    "    except ValueError:\n",
    "        predictions[i*BATCH_SIZE:] = model.predict([x1, x2], batch_size=BATCH_SIZE, verbose=0)[:(df_test.shape[0]-i*BATCH_SIZE)]\n",
    "\n",
    "    i += 1\n",
    "    if (i % 1000 == 0):\n",
    "        print(\"Predicted Batch {0}/{1}, ETC: {2} seconds\".format(i,\n",
    "                                                                int(df_test.shape[0]/BATCH_SIZE),\n",
    "                                                                int(int(df_test.shape[0]/BATCH_SIZE+1)-i)*(time()-t1)))\n",
    "\n",
    "\n",
    "df_test[\"is_duplicate\"] = predictions\n",
    "\n",
    "\n",
    "df_test[['test_id','is_duplicate']].to_csv(SUBMIT_PATH, header=True, index=False)\n",
    "print(\"Done!\")\n",
    "print(\"Submission file saved to:\",check_output([\"ls\", SUBMIT_PATH]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5dfc0b12-1c08-8579-39fe-70feae04652f"
   },
   "source": [
    "The test set preprocessing and predictions are done in batches, just because it does not fit on my pc memory.\n",
    "\n",
    "If there is some flaw on my code or you guys have some comments, I will be glad to listen to it!\n",
    "Thanks for your time!"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 1029,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
